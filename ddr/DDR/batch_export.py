import codecs
from datetime import datetime
import logging
import os

from DDR import config
from DDR import fileio
from DDR import identifier
from DDR import models
from DDR import modules
from DDR import util


def dtfmt(dt, fmt='%Y-%m-%dT%H:%M:%S.%f'):
    """Format dates in consistent format.
    
    >>> dtfmt(datetime.fromtimestamp(0), fmt='%Y-%m-%dT%H:%M:%S.%f')
    '1969-12-31T16:00:00.000000'
    
    @param dt: datetime
    @param fmt: str Format string (default: '%Y-%m-%dT%H:%M:%S.%f')
    @returns: str
    """
    return dt.strftime(fmt)

def make_tmpdir(tmpdir):
    """Make tmp dir if doesn't exist.
    
    @param tmpdir: Absolute path to dir
    """
    if not os.path.exists(tmpdir):
        os.makedirs(tmpdir)

def module_field_names(module):
    """Manipulates list of fieldnames to include/exclude columns from CSV.
    
    >>> m = TestModule()
    >>> m.FIELDS = [{'name':'id'}, {'name':'title'}, {'name':'description'}]
    >>> m.FIELDS_CSV_EXCLUDED = ['description']
    >>> m.MODEL = 'collection'
    >>> batch.module_field_names(m)
    ['id', 'title']
    >>> m.MODEL = 'entity'
    >>> batch.module_field_names(m)
    ['id', 'title']
    >>> m.MODEL = 'file'
    >>> batch.module_field_names(m)
    ['file_id', 'id', 'title']
    
    @param module: 
    @returns: list of field names
    """
    if hasattr(module, 'FIELDS_CSV_EXCLUDED'):
        excluded = module.FIELDS_CSV_EXCLUDED
    else:
        excluded = []
    fields = []
    for field in module.FIELDS:
        if not field['name'] in excluded:
            fields.append(field['name'])
    # TODO don't refer to specific models
    if module.MODEL == 'collection':
        pass
    elif module.MODEL == 'entity':
        pass
    elif module.MODEL == 'file':
        fields.insert(0, 'file_id')
    return fields

def normalize_text(text):
    """Strip text, convert line endings, etc.
    
    TODO make this work on lists, dict values
    TODO handle those ^M chars
    
    >>> normalize_text('  this is a test')
    'this is a test'
    >>> normalize_text('this is a test  ')
    'this is a test'
    >>> normalize_text('this\r\nis a test')
    'this\\nis a test'
    >>> normalize_text('this\ris a test')
    'this\\nis a test'
    >>> normalize_text('this\\nis a test')
    'this\\nis a test'
    >>> normalize_text(['this is a test'])
    ['this is a test']
    >>> normalize_text({'this': 'is a test'})
    {'this': 'is a test'}
    """
    def process(t):
        try:
            t = t.strip()
            t = t.replace('\r\n', '\n').replace('\r', '\n').replace('\n', '\\n')
        except AttributeError:
            pass # doesn't work on ints and lists :P
        return t
    if isinstance(text, basestring):
        return process(text)
    return text

def dump_object(obj, module, field_names):
    """Dump object field values to list.
    
    Note: Autogenerated and non-user-editable fields
    (SHA1 and other hashes, file size, etc) should be excluded
    from the CSV file.
    Note: For files these are replaced by File.id which contains
    the role and a fragment of the SHA1 hash.
    
    @param obj_
    @param module: 
    @param field_names: 
    @returns: list of values
    """
    # seealso DDR.modules.Module.function
    values = []
    for field_name in field_names:
        value = ''
        # insert file_id as first column
        if (module.MODEL == 'file') and (field_name == 'file_id'):
            val = obj.id
        elif hasattr(obj, field_name):
            # run csvdump_* functions on field data if present
            val = modules.Module(module).function(
                'csvdump_%s' % field_name,
                getattr(obj, field_name)
            )
            if val == None:
                val = ''
        value = normalize_text(val)
        values.append(value)
    return values

def export(json_paths, class_, module, csv_path):
    """Write the specified objects' data to CSV.
    
    # entities
    collection_path = '/var/www/media/base/ddr-test-123'
    entity_paths = []
    for path in util.find_meta_files(basedir=collection_path, recursive=True):
        if os.path.basename(path) == 'entity.json':
            entity_paths.append(path)
    csv_path = '/tmp/ddr-test-123-entities.csv'
    export(entity_paths, entity_module, csv_path)
    
    # files
    collection_path = '/var/www/media/base/ddr-test-123'
    file_paths = []
    for path in util.find_meta_files(basedir=collection_path, recursive=True):
        if ('master' in path) or ('mezzanine' in path):
            file_paths.append(path)
    csv_path = '/tmp/ddr-test-123-files.csv'
    export(file_paths, files_module, csv_path)

    @param json_paths: list of .json files
    @param class_: subclass of Entity or File
    @param module: entity_module or files_module
    @param csv_path: Absolute path to CSV data file.
    """
    if module.MODEL == 'file':
        json_paths = models.sort_file_paths(json_paths)
    else:
        json_paths = util.natural_sort(json_paths)
    make_tmpdir(os.path.dirname(csv_path))
    field_names = module_field_names(module)
    # TODO use fileio
    with codecs.open(csv_path, 'wb', 'utf-8') as csvfile:
        writer = fileio.csv_writer(csvfile)
        writer.writerow(field_names)
        for n,json_path in enumerate(json_paths):
            oi = identifier.Identifier(json_path)
            obj = class_.from_identifier(oi)
            logging.info('%s/%s - %s' % (n+1, len(json_paths), obj.id))
            writer.writerow(dump_object(obj, module, field_names))
    return csv_path
