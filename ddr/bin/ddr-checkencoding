#!/usr/bin/env python
#
# This file is part of ddr-cmdln/ddr
#
#  

description = """Checks collection repository for non-UTF-8 chars, lists offending files."""

epilog = """
Example:
    $ ddr-checkencoding git@mits.densho.org:REPO.git /var/www/media/base/temp/REPO ddr-testing-141

Clones collection repo to specified location, loads every .json file in the collection with strict utf-8 encoding,
then removes the directory.  This should surface any UTF-8 encoding problems.

"""

import argparse
import codecs
import ConfigParser
from datetime import datetime
import json
import logging
import os
import shutil
import sys

import chardet
import git

from DDR import batch
from DDR import models
from DDR import CONFIG_FILES, NoConfigError

config = ConfigParser.ConfigParser()
configs_read = config.read(CONFIG_FILES)
if not configs_read:
    raise NoConfigError('No config file!')

REPO_MODELS_PATH = config.get('cmdln','repo_models_path')
if REPO_MODELS_PATH not in sys.path:
    sys.path.append(REPO_MODELS_PATH)

try:
    from repo_models import collection as collectionmodule
    from repo_models import entity as entitymodule
    from repo_models import files as filemodule
except ImportError:
    raise Exception("Could not load repo_models. No Store mounted, no 'ddr' repository, no valid repo_models in 'ddr' repo, or possibly an error in the repo_model module code.")


def out(verbose, text):
    if verbose:
        print(text)

def clone(url, destpath, verbose=False):
    """
    @param url: 
    @param destpath: 
    """
    out(verbose, 'git clone %s %s' % (url, destpath))
    return git.Repo.clone_from(url, destpath)

def clean(repo_path):
    """
    @param repo_path: 
    """
    shutil.rmtree(repo_path)

def read_file(path, verbose=False):
    """Read file in UTF-8 strict, complain if error
    
    open with utf-8 strict
    
    @param path: str
    @param verbose: boolean
    @returns: exit_code,text
    """
    exit_code = 0
    text = None
    try:
        text = models.read_json(path)
    except:
        # TODO print traceback
        exit_code = 1
        raw = models.read_text_unsafe(path)
        guess = chardet.detect(raw)
        if verbose:
            print('|         %s' % (guess))
    return exit_code,text

def load_object(model, path, json_text):
    if model == 'collection':
        o = models.Collection(os.path.dirname(path))
    elif model == 'entity':
        o = models.Entity(os.path.dirname(path))
    elif model == 'file':
        o = models.File(path_abs=path)
    o.load_json(json_text)
    return o
    
def write_file_tmp(obj, path, verbose=False):
    exit_code = 0
    try:
        json_text = obj.dump_json()
        models.write_text(json_text, path)
    except:
        # TODO print traceback
        exit_code = 1
    return exit_code
    
def write_line_csv(module, headers, obj, path, verbose=False):
    exit_code = 0
    try:
        field_names = headers[module.MODEL]
        row = batch.dump_object(obj, module, field_names)
        batch.write_csv(path, field_names, [row])
    except:
        # TODO print traceback
        exit_code = 1
    return exit_code

def analyze_files(modules, module_fields, paths, verbose=False):
    defects = []
    for n,path in enumerate(paths):
        out(verbose, '%s/%s %s' % (n,len(paths),path))
        object_id = models.Identity.id_from_path(path)
        model = models.Identity.model_from_path(path)
        # try to READ file
        exit0,json_text = read_file(path, verbose)
        # load
        obj = None
        if not exit0:
            obj = load_object(model, path, json_text)
        # try to write file
        exit1 = None
        if obj and not exit0:
            path_tmp = '%s-tmp.json' % os.path.splitext(path)[0]
            exit1 = write_file_tmp(obj, path_tmp, verbose)
        # try to write csv
        exit2 = None
        if obj and not (exit0 or exit1):
            module = modules[model]
            headers = module_fields[model]
            path_csv = '%s.csv' % os.path.splitext(path)[0]
            exit2 = write_line_csv(module, headers, obj, path_csv, verbose)
        if exit0 or exit1 or exit2:
            defects.append(path)
    return defects

def analyze(repo_path, verbose=False):
    modules = {
        'collection': collectionmodule,
        'entity': entitymodule,
        'file': filemodule,
    }
    module_fields = {
        'collection': batch.module_field_names(modules['collection']),
        'entity': batch.module_field_names(modules['entity']),
        'file': batch.module_field_names(modules['file']),
    }
    out(verbose, 'Finding metadata files')
    paths = models.metadata_files(repo_path, recursive=True)
    out(verbose, '%s files' % len(paths))
    out(verbose, 'Analyzing files')
    defects = analyze_files(modules, module_fields, paths, verbose)
    return paths,defects


def main():

    parser = argparse.ArgumentParser(description=description, epilog=epilog,
                                     formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('repo_url_pattern', help='Repository URL pattern.')
    parser.add_argument('dest_dir', help='Absolute path to destination dir.')
    parser.add_argument('collection_id', help='Collection ID')
    parser.add_argument(
        '-v', '--verbose', action='store_const', const=1,
        help='Print lots of output. Important lines prefixed with "%%%%".'
    )
    parser.add_argument(
        '-c', '--csv', action='store_const', const=1,
        help='Print output in CSV-friendly form.'
    )
    parser.add_argument(
        '-H', '--headers', action='store_const', const=1,
        help='Print CSV headers (requires -c).'
    )
    parser.add_argument(
        '-j', '--json', action='store_const', const=1,
        help='Print output in JSON-friendly form.'
    )
    
    args = parser.parse_args()
    
    # if verbose, add marker to important lines
    if args.verbose:
        prefix = '%% '
    else:
        prefix = ''
    
    if args.csv and args.headers:
        print('%scollection id, files, defects, elapsed' % prefix)
    
    out(args.verbose, args.collection_id)
    
    url = args.repo_url_pattern.replace('REPO', args.collection_id)
    repo_path = args.dest_dir.replace('REPO', args.collection_id)
    
    start = datetime.now()
    out(args.verbose, start)
    
    repo = clone(url, repo_path, args.verbose)
    out(args.verbose, repo)
    
    paths,defects = analyze(repo_path, args.verbose)
    
    out(args.verbose, 'cleaning up')
    clean(repo_path)
    
    end = datetime.now()
    elapsed = end - start
    out(args.verbose, end)
    
    if args.csv:
        print '%s%s' % (
            prefix,
            ','.join([
                unicode(args.collection_id), unicode(len(paths)), unicode(len(defects)), unicode(elapsed)
            ])
        )
    elif args.json:
        data = {
            'collection id': args.collection_id,
            'files': len(paths),
            'defects': len(defects),
            'elapsed': unicode(elapsed),
            }
        print '%s%s' % (
            prefix,
            json.dumps(data)
        )
    else:
        print('%s%s, %s files, %s bad, %s elapsed' % (
            prefix, args.collection_id, len(paths), len(defects), elapsed))


if __name__ == '__main__':
    main()
